{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework and bake-off: Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Christopher Potts\"\n",
    "__version__ = \"CS224u, Stanford, Spring 2021\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up\n",
    "\n",
    "See [the first notebook in this unit](sst_01_overview.ipynb#Set-up) for set-up instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_rnn_classifier import TorchRNNClassifier\n",
    "from torch_tree_nn import TorchTreeNN\n",
    "import sst\n",
    "import sst_mod\n",
    "from sklearn.metrics import classification_report\n",
    "from iit import get_IIT_sentiment_dataset, get_IIT_sentiment_devset\n",
    "from torch_bert_classifier_IIT import TorchBertClassifierIIT\n",
    "from torch_deep_neural_classifier_iit import TorchDeepNeuralClassifierIIT\n",
    "from torch_rnn_classifier import TorchRNNClassifier\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SST_HOME = os.path.join('data', 'sentiment')\n",
    "LEFT = 0\n",
    "RIGHT = 1\n",
    "BOTH = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Softmax Baseline\n",
    "\n",
    "Sets up two softmax models: one baseline for sentiment analysis, and the other used as the causal backbone for the IIT training, combining the sentiments of two subtrees extracted from SST-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigrams_phi(text):\n",
    "    return Counter(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thin wrapper around `LogisticRegression` for the sake of `sst.experiment`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_softmax_classifier(X, y):\n",
    "    mod = LogisticRegression(\n",
    "        fit_intercept=True,\n",
    "        solver='liblinear',\n",
    "        multi_class='ovr')\n",
    "    mod.fit(X, y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experimental run with some notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assessment dataset 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.628     0.689     0.657       428\n",
      "     neutral      0.343     0.153     0.211       229\n",
      "    positive      0.629     0.750     0.684       444\n",
      "\n",
      "    accuracy                          0.602      1101\n",
      "   macro avg      0.533     0.531     0.518      1101\n",
      "weighted avg      0.569     0.602     0.575      1101\n",
      "\n",
      "Assessment dataset 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.272     0.692     0.391       565\n",
      "     neutral      0.429     0.113     0.179      1019\n",
      "    positive      0.409     0.346     0.375       777\n",
      "\n",
      "    accuracy                          0.328      2361\n",
      "   macro avg      0.370     0.384     0.315      2361\n",
      "weighted avg      0.385     0.328     0.294      2361\n",
      "\n",
      "Mean of macro-F1 scores: 0.416\n"
     ]
    }
   ],
   "source": [
    "softmax_experiment = sst.experiment(\n",
    "    sst.train_reader(SST_HOME),   # Train on any data you like except SST-3 test!\n",
    "    unigrams_phi,                 # Free to write your own!\n",
    "    fit_softmax_classifier,       # Free to write your own!\n",
    "    assess_dataframes=[sst.dev_reader(SST_HOME), sst.bakeoff_dev_reader(SST_HOME)]) # Free to change this during development!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(label):\n",
    "    sents = ['positive', 'neutral', 'negative']\n",
    "    return np.eye(len(sents))[sents.index(label)]\n",
    "\n",
    "def build_dataset_subtrees(dataframes, phi, vectorizer=None, vectorize=True):\n",
    "    if isinstance(dataframes, (list, tuple)):\n",
    "        df = pd.concat(dataframes)\n",
    "    else:\n",
    "        df = dataframes\n",
    "\n",
    "    raw_examples = list(df.sentence.values)\n",
    "\n",
    "    # feat_dicts = list(df.left_label.apply(phi).values)\n",
    "    left_labels = df.left_label.values\n",
    "    right_labels = df.right_label.values\n",
    "\n",
    "    feat_dicts = [np.concatenate((one_hot(left_labels[i]), one_hot(right_labels[i]))) for i in range(len(left_labels))]\n",
    "\n",
    "    if 'sentence_label' in df.columns:\n",
    "        labels = list(df.sentence_label.values)\n",
    "    else:\n",
    "        labels = None\n",
    "\n",
    "    feat_matrix = None\n",
    "    if vectorize:\n",
    "        # In training, we want a new vectorizer:\n",
    "        if vectorizer is None:\n",
    "            vectorizer = DictVectorizer(sparse=False)\n",
    "            feat_matrix = vectorizer.fit_transform(feat_dicts)\n",
    "        # In assessment, we featurize using the existing vectorizer:\n",
    "        else:\n",
    "            feat_matrix = vectorizer.transform(feat_dicts)\n",
    "    else:\n",
    "        feat_matrix = feat_dicts\n",
    "\n",
    "    return {'X': feat_matrix,\n",
    "            'y': labels,\n",
    "            'vectorizer': vectorizer,\n",
    "            'raw_examples': raw_examples}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 8544\n",
      "Dev size: 1101\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      1.000     1.000     1.000       428\n",
      "     neutral      1.000     1.000     1.000       229\n",
      "    positive      1.000     1.000     1.000       444\n",
      "\n",
      "    accuracy                          1.000      1101\n",
      "   macro avg      1.000     1.000     1.000      1101\n",
      "weighted avg      1.000     1.000     1.000      1101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_iit_train_df = pd.read_csv('sst_tree_train.csv')\n",
    "print('Train size:', sentiment_iit_train_df.shape[0])\n",
    "sentiment_iit_dev_df = pd.read_csv('sst_tree_dev.csv')\n",
    "print('Dev size:', sentiment_iit_dev_df.shape[0])\n",
    "\n",
    "# can potentially train softmax on both train and dev datasets, but we'll only focus on train?\n",
    "softmax_tree_experiment = sst_mod.experiment(\n",
    "    sentiment_iit_train_df,\n",
    "    unigrams_phi,\n",
    "    fit_softmax_classifier,\n",
    "    assess_dataframes=[sentiment_iit_dev_df],\n",
    "    vectorize=False,\n",
    "    build_dataset_fn=build_dataset_subtrees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Classifier IIT Training\n",
    "\n",
    "Runs an example of IIT training for sentiment analysis using the `DeepNeuralClassifier` IIT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('positive', 'positive'): 2361, ('positive', 'neutral'): 861, ('positive', 'negative'): 378, ('neutral', 'positive'): 105, ('neutral', 'neutral'): 205, ('neutral', 'negative'): 330, ('negative', 'positive'): 399, ('negative', 'neutral'): 779, ('negative', 'negative'): 982}\n",
      "{('positive', 'positive'): 2025, ('positive', 'negative'): 630, ('positive', 'neutral'): 945, ('neutral', 'neutral'): 180, ('neutral', 'positive'): 192, ('neutral', 'negative'): 268, ('negative', 'neutral'): 720, ('negative', 'positive'): 648, ('negative', 'negative'): 792}\n",
      "torch.Size([12800, 723])\n"
     ]
    }
   ],
   "source": [
    "softmax_root_model = softmax_tree_experiment['model']\n",
    "\n",
    "train_size = 80\n",
    "train_dataset = sentiment_iit_train_df.sample(train_size, replace=False)\n",
    "# X_base_train, X_sources_train, y_base_train, y_IIT_train, interventions_train, vectorizer = \\\n",
    "#     get_IIT_sentiment_dataset(sentiment_iit_train_df.sample(train_size, replace=False), softmax_root_model, LEFT, unigrams_phi)\n",
    "\n",
    "# train on both left and right subtree nodes\n",
    "left_train = get_IIT_sentiment_dataset(train_dataset, softmax_root_model, LEFT, unigrams_phi)\n",
    "vectorizer = left_train[5]\n",
    "right_train = get_IIT_sentiment_dataset(train_dataset, softmax_root_model, RIGHT, unigrams_phi, vectorizer=vectorizer)\n",
    "\n",
    "X_base_train = torch.cat([left_train[0], right_train[0]], dim=0)\n",
    "X_sources_train = [torch.cat([left_train[1][i], right_train[1][i]], dim=0) for i in range(len(left_train[1]))] \n",
    "y_base_train = torch.cat([left_train[2], right_train[2]])\n",
    "y_IIT_train = torch.cat([left_train[3], right_train[3]])\n",
    "interventions_train = torch.cat([left_train[4], right_train[4]])\n",
    "\n",
    "print(X_base_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 198. Training loss did not improve more than tol=1e-05. Final error is 1.092532455921173."
     ]
    }
   ],
   "source": [
    "embedding_dim = X_base_train.shape[1]\n",
    "# similar to our alignment in the IIT accuracy section?\n",
    "# aligning V1 to left side of layer 1, and V2 to the right side\n",
    "# we are defining both as a list with two values -- why not encode it as a single range from 0  to dim * 2?\n",
    "id_to_coords = {LEFT:{1: [{\"layer\":1, \"start\":0, \"end\":embedding_dim}]}, \\\n",
    "    RIGHT: {1: [{\"layer\":1, \"start\":embedding_dim, \"end\":embedding_dim*2}]}, \\\n",
    "    BOTH: {1: [{\"layer\":1, \"start\":0, \"end\":embedding_dim},{\"layer\":1, \"start\":embedding_dim, \"end\":embedding_dim*2}]}}\n",
    "\n",
    "# gives back an IIT dataset based off of the Premack dataset, coming up with \n",
    "# all possible permutations of same/different shape pairs and same/different base-source pairs?\n",
    "# X_base_train, X_sources_train, y_base_train, y_IIT_train, interventions = get_IIT_equality_dataset(\"V1\", embedding_dim ,data_size)\n",
    "\n",
    "# this is a different model from the one we defined in the previous cell, but with a similar idea?\n",
    "model = TorchDeepNeuralClassifierIIT(hidden_dim=embedding_dim*4, hidden_activation=torch.nn.ReLU(), num_layers=3, id_to_coords=id_to_coords)\n",
    "# model.fit() function internally calls on model.create_dataset(), which creates dataset in a way that pairs off\n",
    "# source and base inputs?\n",
    "_ = model.fit(X_base_train, X_sources_train, y_base_train, y_IIT_train, interventions_train)\n",
    "\n",
    "# this is a runtime error I've also encountered in antra (with no change to the original code)\n",
    "# could this be due to mismatching pytorch versions??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trained IIT model\n",
    "# model.to_pickle('deep_neural_classifier_iit.pickle') # pickle seems to throw error for IIT\n",
    "torch.save(model.model.state_dict(), 'deep_neural_classifier_iit.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load IIT model from saved state dictionary\n",
    "# # unpickled = TorchBertClassifierIIT.from_pickle('deep_neural_classifier_iit.pickle')\n",
    "# # not the prettiest, but must construct model by first calling on fit()\n",
    "# model = TorchDeepNeuralClassifierIIT(hidden_dim=embedding_dim*4, hidden_activation=torch.nn.ReLU(), num_layers=3, id_to_coords=id_to_coords)\n",
    "# _ = model.fit(X_base_train, X_sources_train, y_base_train, y_IIT_train, interventions_train)\n",
    "\n",
    "# model.model.load_state_dict(torch.load('deep_neural_classifier_iit.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('negative', 'negative'): 491, ('negative', 'positive'): 13, ('negative', 'neutral'): 16, ('positive', 'positive'): 533, ('neutral', 'negative'): 143, ('neutral', 'positive'): 169, ('neutral', 'neutral'): 208, ('positive', 'negative'): 11, ('positive', 'neutral'): 16}\n",
      "Accuracy of base model\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.36      0.38       560\n",
      "           1       0.43      0.46      0.44       520\n",
      "           2       0.43      0.46      0.44       520\n",
      "\n",
      "    accuracy                           0.42      1600\n",
      "   macro avg       0.42      0.43      0.42      1600\n",
      "weighted avg       0.42      0.42      0.42      1600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tests the IIT model on interchange intervention accuracy using the SST-3 dev set\n",
    "dev_iit_size = 40\n",
    "\n",
    "dev_set_iit = sentiment_iit_dev_df.sample(dev_iit_size, replace=False)\n",
    "\n",
    "# test for intervention on left subtree\n",
    "X_base_dev, X_sources_dev, y_base_dev, y_IIT_dev, interventions_dev, v = \\\n",
    "    get_IIT_sentiment_dataset(dev_set_iit, softmax_root_model, LEFT, unigrams_phi, vectorizer)\n",
    "IIT_preds, base_preds = model.model(model.prep_input(X_base_dev, X_sources_dev, interventions_dev))\n",
    "IIT_preds = np.array(IIT_preds.argmax(axis=1).cpu())\n",
    "base_preds = np.array(base_preds.argmax(axis=1).cpu())\n",
    "print(\"Accuracy of base model\")\n",
    "print(classification_report(y_base_dev, base_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interchange intervention accuracy on left\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.23      0.29       715\n",
      "           1       0.17      0.40      0.24       240\n",
      "           2       0.39      0.38      0.39       645\n",
      "\n",
      "    accuracy                           0.32      1600\n",
      "   macro avg       0.32      0.34      0.31      1600\n",
      "weighted avg       0.36      0.32      0.32      1600\n",
      "\n",
      "{('negative', 'negative'): 321, ('negative', 'positive'): 169, ('positive', 'negative'): 168, ('positive', 'positive'): 377, ('positive', 'neutral'): 15, ('negative', 'neutral'): 30, ('neutral', 'negative'): 156, ('neutral', 'neutral'): 195, ('neutral', 'positive'): 169}\n",
      "--------------------------------------------------------\n",
      "Interchange intervention accuracy on right\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.20      0.26       715\n",
      "           1       0.15      0.37      0.22       240\n",
      "           2       0.37      0.37      0.37       645\n",
      "\n",
      "    accuracy                           0.29      1600\n",
      "   macro avg       0.30      0.31      0.28      1600\n",
      "weighted avg       0.34      0.29      0.30      1600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split across cells for readability of output\n",
    "print(\"Interchange intervention accuracy on left\")\n",
    "print(classification_report(y_IIT_dev, IIT_preds))\n",
    "\n",
    "# test for intervention on right subtree\n",
    "X_base_dev, X_sources_dev, y_base_dev, y_IIT_dev, interventions_dev, v = \\\n",
    "    get_IIT_sentiment_dataset(dev_set_iit, softmax_root_model, RIGHT, unigrams_phi, vectorizer)\n",
    "IIT_preds, base_preds = model.model(model.prep_input(X_base_dev, X_sources_dev, interventions_dev))\n",
    "IIT_preds = np.array(IIT_preds.argmax(axis=1).cpu())\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(\"Interchange intervention accuracy on right\")\n",
    "print(classification_report(y_IIT_dev, IIT_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.56      0.45       777\n",
      "           1       0.49      0.39      0.44      1019\n",
      "           2       0.24      0.16      0.20       565\n",
      "\n",
      "    accuracy                           0.40      2361\n",
      "   macro avg       0.37      0.37      0.36      2361\n",
      "weighted avg       0.39      0.40      0.38      2361\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tests the IIT model on the regular sentiment classification task\n",
    "dev_size = 100\n",
    "dev_dataset = sst.bakeoff_dev_reader(SST_HOME) # .sample(dev_size, replace=False)\n",
    "\n",
    "X_base_dev, X_sources_dev, y_base_dev, y_IIT_dev, interventions_dev = get_IIT_sentiment_devset(\n",
    "    dev_dataset, LEFT, unigrams_phi, vectorizer)\n",
    "\n",
    "IIT_preds, base_preds = model.model(model.prep_input(X_base_dev, X_sources_dev, interventions_dev))\n",
    "base_preds = np.array(base_preds.argmax(axis=1).cpu())\n",
    "print(classification_report(y_base_dev, base_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT IIT Training\n",
    "\n",
    "Builds off of the Deep Neural Classifier IIT Training example, and trains an IIT model based on finetuning BERT for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_fine_tune_phi(text):\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('neutral', 'neutral'): 27, ('neutral', 'negative'): 6, ('neutral', 'positive'): 3, ('negative', 'negative'): 52, ('negative', 'neutral'): 18, ('negative', 'positive'): 2, ('positive', 'positive'): 36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 196. Training loss did not improve more than tol=1e-05. Final error is 0.0006829852700320771."
     ]
    }
   ],
   "source": [
    "dim = 768 # taken from finetuning.ipynb file\n",
    "half_dim = dim // 2\n",
    "\n",
    "layer = 3 # tryyy to go one from the top?\n",
    "id_to_coords = {LEFT:{layer: [{\"layer\":layer, \"start\":0, \"end\":half_dim}]}, \\\n",
    "    RIGHT: {layer: [{\"layer\":layer, \"start\":half_dim, \"end\":dim}]}, \\\n",
    "    BOTH: {layer: [{\"layer\":layer, \"start\":layer, \"end\":half_dim}, {\"layer\":layer, \"start\":half_dim, \"end\":dim}]}}\n",
    "    \n",
    "bert_model = TorchBertClassifierIIT(id_to_coords,\n",
    "                                    n_iter_no_change=5,\n",
    "                                    # max_iter=2,\n",
    "                                    batch_size=8,\n",
    "                                    eta=0.0001)\n",
    "# bert_model = TorchBertClassifierIIT(id_to_coords)\n",
    "\n",
    "train_size = 12 # this is very small, but maybe not toooo small?\n",
    "X_base, X_sources, y_base, y_IIT, interventions, vectorizer = \\\n",
    "    get_IIT_sentiment_dataset(sentiment_iit_train_df.sample(train_size), softmax_root_model, LEFT, \n",
    "                              bert_fine_tune_phi, vectorize=False)\n",
    "\n",
    "_ = bert_model.fit(X_base, X_sources, y_base, y_IIT, interventions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trained IIT model\n",
    "# model.to_pickle('deep_neural_classifier_iit.pickle') # pickle seems to throw error for IIT\n",
    "torch.save(bert_model.model.state_dict(), 'bert_classifier_iit.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('neutral', 'neutral'): 16, ('neutral', 'negative'): 12, ('neutral', 'positive'): 12, ('negative', 'negative'): 30, ('positive', 'positive'): 30}\n",
      "Accuracy of base model\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        30\n",
      "           1       0.42      0.25      0.31        40\n",
      "           2       0.25      0.60      0.35        30\n",
      "\n",
      "    accuracy                           0.28       100\n",
      "   macro avg       0.22      0.28      0.22       100\n",
      "weighted avg       0.24      0.28      0.23       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tests the IIT model on interchange intervention accuracy using the SST-3 dev set\n",
    "dev_iit_size = 10\n",
    "dev_set_iit = sentiment_iit_dev_df.sample(dev_iit_size, replace=False)\n",
    "\n",
    "# test for intervention on left subtree\n",
    "X_base_dev, X_sources_dev, y_base_dev, y_IIT_dev, interventions_dev, v = \\\n",
    "    get_IIT_sentiment_dataset(dev_set_iit, softmax_root_model, LEFT, bert_fine_tune_phi, vectorize=False)\n",
    "IIT_preds, base_preds = bert_model.model(bert_model.prep_input(X_base_dev, X_sources_dev, interventions_dev))\n",
    "IIT_preds = np.array(IIT_preds.argmax(axis=1).cpu())\n",
    "base_preds = np.array(base_preds.argmax(axis=1).cpu())\n",
    "print(\"Accuracy of base model\")\n",
    "print(classification_report(y_base_dev, base_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interchange intervention accuracy on left\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        42\n",
      "           1       0.12      0.25      0.17        16\n",
      "           2       0.41      0.67      0.51        42\n",
      "\n",
      "    accuracy                           0.32       100\n",
      "   macro avg       0.18      0.31      0.23       100\n",
      "weighted avg       0.19      0.32      0.24       100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amirz\\anaconda3\\envs\\iit\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\amirz\\anaconda3\\envs\\iit\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\amirz\\anaconda3\\envs\\iit\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('neutral', 'neutral'): 16, ('neutral', 'negative'): 12, ('neutral', 'positive'): 12, ('negative', 'negative'): 21, ('negative', 'positive'): 9, ('positive', 'positive'): 21, ('positive', 'negative'): 9}\n",
      "--------------------------------------------------------\n",
      "Interchange intervention accuracy on right\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        42\n",
      "           1       0.08      0.12      0.10        16\n",
      "           2       0.38      0.67      0.49        42\n",
      "\n",
      "    accuracy                           0.30       100\n",
      "   macro avg       0.15      0.26      0.19       100\n",
      "weighted avg       0.17      0.30      0.22       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split across cells for readability of output\n",
    "print(\"Interchange intervention accuracy on left\")\n",
    "print(classification_report(y_IIT_dev, IIT_preds))\n",
    "\n",
    "# test for intervention on right subtree\n",
    "X_base_dev, X_sources_dev, y_base_dev, y_IIT_dev, interventions_dev, v = \\\n",
    "    get_IIT_sentiment_dataset(dev_set_iit, softmax_root_model, RIGHT, bert_fine_tune_phi, vectorize=False)\n",
    "IIT_preds, base_preds = bert_model.model(bert_model.prep_input(X_base_dev, X_sources_dev, interventions_dev))\n",
    "IIT_preds = np.array(IIT_preds.argmax(axis=1).cpu())\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(\"Interchange intervention accuracy on right\")\n",
    "print(classification_report(y_IIT_dev, IIT_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.07      0.13        42\n",
      "           1       0.40      0.69      0.51        36\n",
      "           2       0.27      0.41      0.33        22\n",
      "\n",
      "    accuracy                           0.37       100\n",
      "   macro avg       0.47      0.39      0.32       100\n",
      "weighted avg       0.52      0.37      0.31       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate IIT model on bakeoff dev set, measuring regular accuracy\n",
    "dev_size = 100\n",
    "\n",
    "X_base_dev, X_sources_dev, y_base_dev, y_IIT_dev, interventions_dev  = \\\n",
    "    get_IIT_sentiment_devset(sst.bakeoff_dev_reader(SST_HOME).sample(dev_size, replace=False), LEFT, bert_fine_tune_phi, None, False)\n",
    "\n",
    "y_predict, y_IIT_predict = bert_model.model(bert_model.prep_input(X_base_dev, X_sources_dev, interventions_dev))\n",
    "y_predict = np.array(y_predict.argmax(axis=1).cpu())\n",
    "print(classification_report(y_base_dev, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sandbox\n",
    "\n",
    "Just a scratch workspace for BERT IIT training and paramater seach (one day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test BERT model on small example\n",
    "\n",
    "# base = ['This is just a single test']\n",
    "# sources = [base]\n",
    "# coord_ids = [0] * len(base)\n",
    "\n",
    "# LABELS = ['positive', 'neutral', 'negative']\n",
    "# _, y_ = bert_model.model(bert_model.prep_input(base, sources, coord_ids))\n",
    "# y_ = np.array(y_.argmax(axis=1).cpu())\n",
    "# y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fit_iit_bert_classifier_with_hyperparameter_search(X, y):\n",
    "#     basemod = TorchBertClassifierIIT(\n",
    "#         weights_name='bert-base-cased',\n",
    "#         batch_size=8,  # Small batches to avoid memory overload.\n",
    "#         max_iter=1,  # We'll search based on 1 iteration for efficiency.\n",
    "#         n_iter_no_change=5,   # Early-stopping params are for the\n",
    "#         early_stopping=True)  # final evaluation.\n",
    "\n",
    "#     param_grid = {\n",
    "#         'gradient_accumulation_steps': [1, 4, 8],\n",
    "#         'eta': [0.00005, 0.0001, 0.001]}\n",
    "\n",
    "#     bestmod = utils.fit_classifier_with_hyperparameter_search(\n",
    "#         X, y, basemod, cv=3, param_grid=param_grid)\n",
    "\n",
    "#     return bestmod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# bert_classifier_xval = sst.experiment(\n",
    "#     sst.train_reader(SST_HOME),\n",
    "#     bert_fine_tune_phi,\n",
    "#     fit_iit_bert_classifier_with_hyperparameter_search,\n",
    "#     assess_dataframes=sst.dev_reader(SST_HOME),\n",
    "#     vectorize=False)  # Pass in the BERT hidden state directly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimized_bert_classifier = bert_classifier_xval['model']\n",
    "# del bert_classifier_xval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fit_optimized_hf_bert_classifier(X, y):\n",
    "#     optimized_bert_classifier.max_iter = 1000\n",
    "#     optimized_bert_classifier.fit(X, y)\n",
    "#     return optimized_bert_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# _ = sst.experiment(\n",
    "#     sst.train_reader(SST_HOME),\n",
    "#     bert_fine_tune_phi,\n",
    "#     fit_optimized_hf_bert_classifier,\n",
    "#     assess_dataframes=test_df,\n",
    "#     vectorize=False)  # Pass in the BERT hidden state directly!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
