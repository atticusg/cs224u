{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework and bake-off: Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Christopher Potts\"\n",
    "__version__ = \"CS224u, Stanford, Spring 2021\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Overview](#Overview)\n",
    "1. [Methodological note](#Methodological-note)\n",
    "1. [Set-up](#Set-up)\n",
    "1. [Train set](#Train-set)\n",
    "1. [Dev sets](#Dev-sets)\n",
    "1. [A softmax baseline](#A-softmax-baseline)\n",
    "1. [RNNClassifier wrapper](#RNNClassifier-wrapper)\n",
    "1. [Error analysis](#Error-analysis)\n",
    "1. [Homework questions](#Homework-questions)\n",
    "  1. [Token-level differences [1 point]](#Token-level-differences-[1-point])\n",
    "  1. [Training on some of the bakeoff data [1 point]](#Training-on-some-of-the-bakeoff-data-[1-point])\n",
    "  1. [A more powerful vector-averaging baseline [2 points]](#A-more-powerful-vector-averaging-baseline-[2-points])\n",
    "  1. [BERT encoding [2 points]](#BERT-encoding-[2-points])\n",
    "  1. [Your original system [3 points]](#Your-original-system-[3-points])\n",
    "1. [Bakeoff [1 point]](#Bakeoff-[1-point])\n",
    "1. [Submission Instruction](#Submission-Instruction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This homework and associated bakeoff are devoted to supervised sentiment analysis using the ternary (positive/negative/neutral) version of the Stanford Sentiment Treebank (SST-3) as well as a new dev/test dataset drawn from restaurant reviews. Our goal in introducing the new dataset is to push you to create a system that performs well in both the movie and restaurant domains.\n",
    "\n",
    "The homework questions ask you to implement some baseline system, and the bakeoff challenge is to define a system that does well at both the SST-3 test set and the new restaurant test set. Both are ternary tasks, and our central bakeoff score is the mean of the macro-FI scores for the two datasets. This assigns equal weight to all classes and datasets regardless of size.\n",
    "\n",
    "The SST-3 test set will be used for the bakeoff evaluation. This dataset is already publicly distributed, so we are counting on people not to cheat by developing their models on the test set. You must do all your development without using the test set at all, and then evaluate exactly once on the test set and turn in the results, with no further system tuning or additional runs. __Much of the scientific integrity of our field depends on people adhering to this honor code__. \n",
    "\n",
    "One of our goals for this homework and bakeoff is to encourage you to engage in __the basic development cycle for supervised models__, in which you\n",
    "\n",
    "1. Design a new system. We recommend starting with something simple.\n",
    "1. Use `sst.experiment` to evaluate your system, using random train/test splits initially.\n",
    "1. If you have time, compare your system with others using `sst.compare_models` or `utils.mcnemar`. (For discussion, see [this notebook section](sst_02_hand_built_features.ipynb#Statistical-comparison-of-classifier-models).)\n",
    "1. Return to step 1, or stop the cycle and conduct a more rigorous evaluation with hyperparameter tuning and assessment on the `dev` set.\n",
    "\n",
    "[Error analysis](#Error-analysis) is one of the most important methods for steadily improving a system, as it facilitates a kind of human-powered hill-climbing on your ultimate objective. Often, it takes a careful human analyst just a few examples to spot a major pattern that can lead to a beneficial change to the feature representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up\n",
    "\n",
    "See [the first notebook in this unit](sst_01_overview.ipynb#Set-up) for set-up instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_rnn_classifier import TorchRNNClassifier\n",
    "from torch_tree_nn import TorchTreeNN\n",
    "import sst\n",
    "import sst_mod\n",
    "from sklearn.metrics import classification_report\n",
    "from iit import get_IIT_sentiment_dataset, get_IIT_sentiment_devset\n",
    "from torch_bert_classifier_IIT import TorchBertClassifierIIT\n",
    "from torch_deep_neural_classifier_iit import TorchDeepNeuralClassifierIIT\n",
    "from torch_rnn_classifier import TorchRNNClassifier\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SST_HOME = os.path.join('data', 'sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A softmax baseline\n",
    "\n",
    "This example is here mainly as a reminder of how to use our experimental framework with linear models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigrams_phi(text):\n",
    "    return Counter(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thin wrapper around `LogisticRegression` for the sake of `sst.experiment`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_softmax_classifier(X, y):\n",
    "    mod = LogisticRegression(\n",
    "        fit_intercept=True,\n",
    "        solver='liblinear',\n",
    "        multi_class='ovr')\n",
    "    mod.fit(X, y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experimental run with some notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assessment dataset 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.628     0.689     0.657       428\n",
      "     neutral      0.343     0.153     0.211       229\n",
      "    positive      0.629     0.750     0.684       444\n",
      "\n",
      "    accuracy                          0.602      1101\n",
      "   macro avg      0.533     0.531     0.518      1101\n",
      "weighted avg      0.569     0.602     0.575      1101\n",
      "\n",
      "Assessment dataset 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.272     0.692     0.391       565\n",
      "     neutral      0.429     0.113     0.179      1019\n",
      "    positive      0.409     0.346     0.375       777\n",
      "\n",
      "    accuracy                          0.328      2361\n",
      "   macro avg      0.370     0.384     0.315      2361\n",
      "weighted avg      0.385     0.328     0.294      2361\n",
      "\n",
      "Mean of macro-F1 scores: 0.416\n"
     ]
    }
   ],
   "source": [
    "softmax_experiment = sst.experiment(\n",
    "    sst.train_reader(SST_HOME),   # Train on any data you like except SST-3 test!\n",
    "    unigrams_phi,                 # Free to write your own!\n",
    "    fit_softmax_classifier,       # Free to write your own!\n",
    "    assess_dataframes=[sst.dev_reader(SST_HOME), sst.bakeoff_dev_reader(SST_HOME)]) # Free to change this during development!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(label):\n",
    "    sents = ['positive', 'neutral', 'negative']\n",
    "    return np.eye(len(sents))[sents.index(label)]\n",
    "\n",
    "def build_dataset_subtrees(dataframes, phi, vectorizer=None, vectorize=True):\n",
    "    if isinstance(dataframes, (list, tuple)):\n",
    "        df = pd.concat(dataframes)\n",
    "    else:\n",
    "        df = dataframes\n",
    "\n",
    "    raw_examples = list(df.sentence.values)\n",
    "\n",
    "    # feat_dicts = list(df.left_label.apply(phi).values)\n",
    "    left_labels = df.left_label.values\n",
    "    right_labels = df.right_label.values\n",
    "\n",
    "    feat_dicts = [np.concatenate((one_hot(left_labels[i]), one_hot(right_labels[i]))) for i in range(len(left_labels))]\n",
    "\n",
    "    if 'sentence_label' in df.columns:\n",
    "        labels = list(df.sentence_label.values)\n",
    "    else:\n",
    "        labels = None\n",
    "\n",
    "    feat_matrix = None\n",
    "    if vectorize:\n",
    "        # In training, we want a new vectorizer:\n",
    "        if vectorizer is None:\n",
    "            vectorizer = DictVectorizer(sparse=False)\n",
    "            feat_matrix = vectorizer.fit_transform(feat_dicts)\n",
    "        # In assessment, we featurize using the existing vectorizer:\n",
    "        else:\n",
    "            feat_matrix = vectorizer.transform(feat_dicts)\n",
    "    else:\n",
    "        feat_matrix = feat_dicts\n",
    "\n",
    "    return {'X': feat_matrix,\n",
    "            'y': labels,\n",
    "            'vectorizer': vectorizer,\n",
    "            'raw_examples': raw_examples}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      1.000     1.000     1.000       428\n",
      "     neutral      1.000     1.000     1.000       229\n",
      "    positive      1.000     1.000     1.000       444\n",
      "\n",
      "    accuracy                          1.000      1101\n",
      "   macro avg      1.000     1.000     1.000      1101\n",
      "weighted avg      1.000     1.000     1.000      1101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_iit_train_df = pd.read_csv('sst_tree_train.csv')\n",
    "sentiment_iit_dev_df = pd.read_csv('sst_tree_dev.csv')\n",
    "\n",
    "softmax_tree_experiment = sst_mod.experiment(\n",
    "    sentiment_iit_train_df,\n",
    "    unigrams_phi,\n",
    "    fit_softmax_classifier,\n",
    "    assess_dataframes=[sentiment_iit_dev_df],\n",
    "    vectorize=False,\n",
    "    build_dataset_fn=build_dataset_subtrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('positive', 'positive'): 40, ('negative', 'negative'): 56, ('negative', 'positive'): 4}\n",
      "torch.Size([80, 162])\n"
     ]
    }
   ],
   "source": [
    "LEFT = 0\n",
    "RIGHT = 1\n",
    "softmax_root_model = softmax_tree_experiment['model']\n",
    "\n",
    "data_size = 10\n",
    "X_base, X_sources, y_base, y_IIT, interventions, vectorizer = get_IIT_sentiment_dataset(sentiment_iit_dev_df.sample(data_size), softmax_root_model, LEFT, unigrams_phi)\n",
    "\n",
    "split = (data_size * data_size) // 5\n",
    "\n",
    "X_base_train = X_base[split:]\n",
    "X_sources_train = [source[split:] for source in X_sources]\n",
    "y_base_train = y_base[split:]\n",
    "y_IIT_train = y_IIT[split:]\n",
    "interventions_train = interventions[split:]\n",
    "\n",
    "\n",
    "print(X_base_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 33. Training loss did not improve more than tol=1e-05. Final error is 2.714747324716882e-06."
     ]
    }
   ],
   "source": [
    "embedding_dim = X_base.shape[1]\n",
    "V1 = 0\n",
    "V2 = 1\n",
    "both = 2\n",
    "# similar to our alignment in the IIT accuracy section?\n",
    "# aligning V1 to left side of layer 1, and V2 to the right side\n",
    "# we are defining both as a list with two values -- why not encode it as a single range from 0  to dim * 2?\n",
    "id_to_coords = {V1:{1: [{\"layer\":1, \"start\":0, \"end\":embedding_dim}]}, \\\n",
    "    V2: {1: [{\"layer\":1, \"start\":embedding_dim, \"end\":embedding_dim*2}]}, \\\n",
    "    both: {1: [{\"layer\":1, \"start\":0, \"end\":embedding_dim},{\"layer\":1, \"start\":embedding_dim, \"end\":embedding_dim*2}]}}\n",
    "\n",
    "# gives back an IIT dataset based off of the Premack dataset, coming up with \n",
    "# all possible permutations of same/different shape pairs and same/different base-source pairs?\n",
    "# X_base_train, X_sources_train, y_base_train, y_IIT_train, interventions = get_IIT_equality_dataset(\"V1\", embedding_dim ,data_size)\n",
    "\n",
    "# this is a different model from the one we defined in the previous cell, but with a similar idea?\n",
    "model = TorchDeepNeuralClassifierIIT(hidden_dim=embedding_dim*4, hidden_activation=torch.nn.ReLU(), num_layers=3, id_to_coords=id_to_coords)\n",
    "# model.fit() function internally calls on model.create_dataset(), which creates dataset in a way that pairs off\n",
    "# source and base inputs?\n",
    "_ = model.fit(X_base_train, X_sources_train, y_base_train, y_IIT_train, interventions_train)\n",
    "\n",
    "# this is a runtime error I've also encountered in antra (with no change to the original code)\n",
    "# could this be due to mismatching pytorch versions??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         6\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00        14\n",
      "\n",
      "    accuracy                           0.30        20\n",
      "   macro avg       0.33      0.33      0.33        20\n",
      "weighted avg       0.30      0.30      0.30        20\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         8\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00        12\n",
      "\n",
      "    accuracy                           0.40        20\n",
      "   macro avg       0.33      0.33      0.33        20\n",
      "weighted avg       0.40      0.40      0.40        20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amirz\\anaconda3\\envs\\iit\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\amirz\\anaconda3\\envs\\iit\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\amirz\\anaconda3\\envs\\iit\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\amirz\\anaconda3\\envs\\iit\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\amirz\\anaconda3\\envs\\iit\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\amirz\\anaconda3\\envs\\iit\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\amirz\\anaconda3\\envs\\iit\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\amirz\\anaconda3\\envs\\iit\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\amirz\\anaconda3\\envs\\iit\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\amirz\\anaconda3\\envs\\iit\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\amirz\\anaconda3\\envs\\iit\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\amirz\\anaconda3\\envs\\iit\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "X_base_test = X_base[:split]\n",
    "X_sources_test = [source[:split] for source in X_sources]\n",
    "y_base_test = y_base[:split]\n",
    "y_IIT_test = y_IIT[:split]\n",
    "interventions_test = interventions[:split]\n",
    "\n",
    "IIT_preds, base_preds = model.model(model.prep_input(X_base_test, X_sources_test, interventions_test))\n",
    "IIT_preds = np.array(IIT_preds.argmax(axis=1).cpu())\n",
    "base_preds = np.array(base_preds.argmax(axis=1).cpu())\n",
    "print(classification_report(y_base_test, base_preds))\n",
    "print(classification_report(y_IIT_test, IIT_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.37      0.36       777\n",
      "           1       0.46      0.71      0.56      1019\n",
      "           2       0.00      0.00      0.00       565\n",
      "\n",
      "    accuracy                           0.43      2361\n",
      "   macro avg       0.27      0.36      0.31      2361\n",
      "weighted avg       0.32      0.43      0.36      2361\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amirz\\anaconda3\\envs\\iit\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\amirz\\anaconda3\\envs\\iit\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\amirz\\anaconda3\\envs\\iit\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "X_base_dev, X_sources_dev, y_base_dev, y_IIT_dev, interventions_dev = get_IIT_sentiment_devset(\n",
    "    sst.bakeoff_dev_reader(SST_HOME), LEFT, unigrams_phi, vectorizer)\n",
    "\n",
    "IIT_preds, base_preds = model.model(model.prep_input(X_base_dev, X_sources_dev, interventions_dev))\n",
    "base_preds = np.array(base_preds.argmax(axis=1).cpu())\n",
    "print(classification_report(y_base_dev, base_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT IIT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_fine_tune_phi(text):\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('neutral', 'neutral'): 2, ('negative', 'neutral'): 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 18. Training loss did not improve more than tol=1e-05. Final error is 1.1446082592010498."
     ]
    }
   ],
   "source": [
    "LEFT = 0\n",
    "RIGHT = 1\n",
    "BOTH = 2\n",
    "dim = 768\n",
    "half_dim = dim // 2\n",
    "\n",
    "id_to_coords = {LEFT:{3: [{\"layer\":0, \"start\":0, \"end\":half_dim}]}, \\\n",
    "    RIGHT: {3: [{\"layer\":0, \"start\":half_dim, \"end\":dim}]}, \\\n",
    "    BOTH: {3: [{\"layer\":0, \"start\":0, \"end\":half_dim},{\"layer\":0, \"start\":half_dim, \"end\":dim}]}}\n",
    "    \n",
    "test_model = TorchBertClassifierIIT(id_to_coords)\n",
    "\n",
    "data_size = 2\n",
    "X_base, X_sources, y_base, y_IIT, interventions, vectorizer = get_IIT_sentiment_dataset(sentiment_iit_train_df.sample(data_size), softmax_root_model, LEFT, bert_fine_tune_phi, vectorize=False)\n",
    "\n",
    "_ = test_model.fit(X_base, X_sources, y_base, y_IIT, interventions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_base_test, X_sources_test, y_base_test, y_IIT_test, interventions_test  = get_IIT_sentiment_devset(sst.dev_reader(SST_HOME), LEFT, bert_fine_tune_phi, None, False)\n",
    "\n",
    "y_predict, y_IIT_predict = test_model.model(test_model.prep_input(X_base_test, X_sources_test, interventions_test))\n",
    "\n",
    "print(classification_report(y_base_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base = ['This is just a single test']\n",
    "sources = [base]\n",
    "coord_ids = [0] * len(base)\n",
    "\n",
    "LABELS = ['positive', 'neutral', 'negative']\n",
    "_, y_ = test_model.model(test_model.prep_input(base, sources, coord_ids))\n",
    "y_ = np.array(y_.argmax(axis=1).cpu())\n",
    "y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_iit_bert_classifier_with_hyperparameter_search(X, y):\n",
    "    basemod = TorchBertClassifierIIT(\n",
    "        weights_name='bert-base-cased',\n",
    "        batch_size=8,  # Small batches to avoid memory overload.\n",
    "        max_iter=1,  # We'll search based on 1 iteration for efficiency.\n",
    "        n_iter_no_change=5,   # Early-stopping params are for the\n",
    "        early_stopping=True)  # final evaluation.\n",
    "\n",
    "    param_grid = {\n",
    "        'gradient_accumulation_steps': [1, 4, 8],\n",
    "        'eta': [0.00005, 0.0001, 0.001]}\n",
    "\n",
    "    bestmod = utils.fit_classifier_with_hyperparameter_search(\n",
    "        X, y, basemod, cv=3, param_grid=param_grid)\n",
    "\n",
    "    return bestmod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bert_classifier_xval = sst.experiment(\n",
    "    sst.train_reader(SST_HOME),\n",
    "    bert_fine_tune_phi,\n",
    "    fit_iit_bert_classifier_with_hyperparameter_search,\n",
    "    assess_dataframes=sst.dev_reader(SST_HOME),\n",
    "    vectorize=False)  # Pass in the BERT hidden state directly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_bert_classifier = bert_classifier_xval['model']\n",
    "del bert_classifier_xval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_optimized_hf_bert_classifier(X, y):\n",
    "    optimized_bert_classifier.max_iter = 1000\n",
    "    optimized_bert_classifier.fit(X, y)\n",
    "    return optimized_bert_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_ = sst.experiment(\n",
    "    sst.train_reader(SST_HOME),\n",
    "    bert_fine_tune_phi,\n",
    "    fit_optimized_hf_bert_classifier,\n",
    "    assess_dataframes=test_df,\n",
    "    vectorize=False)  # Pass in the BERT hidden state directly!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
